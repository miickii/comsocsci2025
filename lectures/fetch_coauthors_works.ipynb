{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "#############################################\n",
    "# UTILITY FUNCTIONS\n",
    "#############################################\n",
    "\n",
    "def batch_list(lst, batch_size):\n",
    "    \"\"\"Yield successive chunks of size batch_size from lst.\"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i+batch_size]\n",
    "\n",
    "#############################################\n",
    "# API FETCH FUNCTIONS\n",
    "#############################################\n",
    "\n",
    "def fetch_works_for_batch(author_ids, filter_params=\"\", per_page=200, sleep_time=0.2):\n",
    "    \"\"\"\n",
    "    Fetch works from OpenAlex for a batch of author IDs.\n",
    "    \n",
    "    Parameters:\n",
    "      - author_ids: list of author OpenAlex IDs (e.g. \"https://openalex.org/A123456789\")\n",
    "      - filter_params: additional filter string (e.g. \"cited_by_count:>10,authorship_count:<10\")\n",
    "      - per_page: number of works per page (max 200)\n",
    "      - sleep_time: pause between pages (to respect rate limits)\n",
    "      \n",
    "    Returns:\n",
    "      - List of work dictionaries.\n",
    "    \"\"\"\n",
    "    works_endpoint = \"https://api.openalex.org/works\"\n",
    "    all_works = []\n",
    "    # Build filter for author IDs (using the OR operator \"|\")\n",
    "    author_filter = \"authorships.author.id:\" + \"|\".join(author_ids)\n",
    "    full_filter = author_filter + (\",\" + filter_params if filter_params else \"\")\n",
    "    \n",
    "    cursor = \"*\"\n",
    "    while True:\n",
    "        params = {\n",
    "            \"filter\": full_filter,\n",
    "            \"per_page\": per_page,\n",
    "            \"cursor\": cursor,\n",
    "            \"select\": \"id,publication_year,cited_by_count,authorships,abstract_inverted_index,title,concepts\"\n",
    "        }\n",
    "        response = requests.get(works_endpoint, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching works for authors {author_ids}: {response.status_code}\")\n",
    "            break\n",
    "        data = response.json()\n",
    "        results = data.get(\"results\", [])\n",
    "        all_works.extend(results)\n",
    "        meta = data.get(\"meta\", {})\n",
    "        next_cursor = meta.get(\"next_cursor\")\n",
    "        if not next_cursor:\n",
    "            break\n",
    "        cursor = next_cursor\n",
    "        time.sleep(sleep_time)\n",
    "    return all_works\n",
    "\n",
    "def fetch_works_for_authors(author_ids, filter_params=\"\", batch_size=25, n_jobs=5):\n",
    "    \"\"\"\n",
    "    Given a list of author IDs, split them into batches and fetch works in parallel.\n",
    "    \n",
    "    Returns:\n",
    "      - List of work dictionaries.\n",
    "    \"\"\"\n",
    "    batches = list(batch_list(author_ids, batch_size))\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(fetch_works_for_batch)(batch, filter_params) for batch in tqdm(batches, desc=\"Fetching works for authors\")\n",
    "    )\n",
    "    # Flatten list of lists\n",
    "    all_works = [work for sublist in results for work in sublist]\n",
    "    return all_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10479"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic2s2_authors_df = pd.read_csv(\"files/ic2s2_coauthors.csv\")\n",
    "\n",
    "ic2s2_authors_df = ic2s2_authors_df[(ic2s2_authors_df[\"Works Count\"] >= 5) & (ic2s2_authors_df[\"Works Count\"] <= 5000)]\n",
    "ic2s2_author_ids = ic2s2_authors_df[\"ID\"].tolist()\n",
    "ic2s2_author_ids = [author_id.split(\"org/\")[1] for author_id in ic2s2_author_ids]\n",
    "\n",
    "# all_works = fetch_works_for_authors(ic2s2_author_ids, filter_params=\"cited_by_count:>10\", batch_size=25, n_jobs=5)\n",
    "# print(f\"Total works fetched for IC2S2 authors: {len(all_works)}\")\n",
    "len(ic2s2_author_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
